% Copyright 2004 by Till Tantau <tantau@users.sourceforge.net>.
%
% In principle, this file can be redistributed and/or modified under
% the terms of the GNU Public License, version 2.
%
% However, this file is supposed to be a template to be modified
% for your own needs. For this reason, if you use this file as a
% template and not specifically distribute it as part of a another
% package/program, I grant the extra permission to freely copy and
% modify this file as you see fit and even to delete this copyright
% notice. 

\documentclass[aspectratio=169]{beamer}
%\documentclass{beamer}

\setbeamersize{text margin left=5mm, text margin right=5mm}


\defbeamertemplate{headline}{my header}{%
\vskip1pt%
\makebox[0pt][l]{\,\insertshortauthor}%
\hspace*{\fill}\insertshorttitle/\insertshortsubtitle\hspace*{\fill}%
\llap{\insertpagenumber/\insertpresentationendpage\,}
}
\setbeamertemplate{headline}[my header]

\let\olditem\item
\renewcommand{\item}{\setlength{\itemsep}{\fill}\olditem}

\usepackage{soul}
\usepackage{tkz-euclide}
\usetikzlibrary{calc}
\usepackage[]{algorithm2e}
\usepackage{changepage}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{mathtools}
\usepackage{tcolorbox}
\usepackage{tikz}
\usepackage{tikz-3dplot}

% \usepackage[math]{cellspace}
% \cellspacetoplimit 4pt
% \cellspacebottomlimit 4pt
%\usetikzlibrary{arrows.meta}

%\setbeamertemplate{itemize items}{-}

%\usepackage{helvet}
\usefonttheme{professionalfonts} % using non standard fonts for beamer
%\usefonttheme{serif} % default family is serif
%\usepackage{fontspec}
%\setmainfont{Liberation Serif}

% There are many different themes available for Beamer. A comprehensive
% list with examples is given here:
% http://deic.uab.es/~iblanes/beamer_gallery/index_by_theme.html
% You can uncomment the themes below if you would like to use a different
% one:
%\usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
%\usetheme{Berkeley}
%\usetheme{Berlin}
%\usetheme{Boadilla}
%\usetheme{boxes}
%\usetheme{CambridgeUS}
%\usetheme{Copenhagen}
%\usetheme{Darmstadt}
%\usetheme{default}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
%\usetheme{Madrid}
%\usetheme{Malmoe}
%\usetheme{Marburg}
%\usetheme{Montpellier}
%\usetheme{PaloAlto}
%\usetheme{Pittsburgh}
%\usetheme{Rochester}
%\usetheme{Singapore}
%\usetheme{Szeged}
%\usetheme{Warsaw}


\def\mf{\ensuremath\mathbf}
\def\mb{\ensuremath\mathbb}
\def\lp{\ensuremath\left(}
\def\rp{\ensuremath\right)}
\def\lv{\ensuremath\left\lvert}
\def\rv{\ensuremath\right\rvert}
\def\lV{\ensuremath\left\lVert}
\def\rV{\ensuremath\right\rVert}
\def\lc{\ensuremath\left\{}
\def\rc{\ensuremath\right\}}
\def\ls{\ensuremath\left[}
\def\rs{\ensuremath\right]}
\def\bmx{\ensuremath\begin{bmatrix*}[r]}
\def\emx{\ensuremath\end{bmatrix*}}
\def\bmxc{\ensuremath\begin{bmatrix*}[c]}
\def\t{\lp t\rp}
\def\k{\ls k\rs}


\newcommand{\demoex}[2]{\onslide<#1->\begin{color}{black!60} #2 \end{color}}
\newcommand{\demoexc}[3]{\onslide<#1->\begin{color}{#2} #3 \end{color}}
\newcommand{\anim}[3]{\onslide<#1->{\begin{color}{#2!60} #3 \end{color}}}
\newcommand{\ct}[1]{\lp #1\rp}
\newcommand{\dt}[1]{\ls #1\rs}
\newcommand{\cols}[2]{\begin{columns}[#1] #2 \end{columns}}
\newcommand{\col}[2]{\begin{column}{#1} #2 \end{column}}


\title{Linear Systems}

% A subtitle is optional and this may be deleted
\subtitle{Matrices}

\author{Sivakumar Balasubramanian}
% - Give the names in the same order as the appear in the paper.
% - Use the \inst{?} command only if the authors have different
%   affiliation.

\institute[Christian Medical College] % (optional, but mostly needed)
{
  \inst{}%
  Department of Bioengineering\\
  Christian Medical College, Bagayam\\
  Vellore 632002
}
% - Use the \inst command only if there are several affiliations.
% - Keep it simple, no one is interested in your street address.

\date{}
% - Either use conference name or its abbreviation.
% - Not really informative to the audience, more for people (including
%   yourself) who are reading the slides online

\subject{Lecture notes on linear systems}
% This is only inserted into the PDF information catalog. Can be left
% out. 

% If you have a file called "university-logo-filename.xxx", where xxx
% is a graphic format that can be processed by latex or pdflatex,
% resp., then you can add a logo as follows:

% \pgfdeclareimage[height=0.5cm]{university-logo}{university-logo-filename}
% \logo{\pgfuseimage{university-logo}}

% Delete this, if you do not want the table of contents to pop up at
% the beginning of each subsection:
\AtBeginSubsection[]
{
  \begin{frame}<beamer>{Outline}
    \tableofcontents[currentsection,currentsubsection]
  \end{frame}
}

% Let's get started
\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}[t]{References}
\begin{itemize}
\item S Boyd, Applied Linear Algebra: Chapters 6, 7, 8, 10 and 11.
\item G Strang, Linear Algebra: Chapters 1 and 2.
\end{itemize}
\end{frame}

\begin{frame}[t]{Matrices}
\begin{itemize}
\item \textbf{Matrices} are rectangular array of numbers. $\begin{bmatrix}
1.1 & -24 & \sqrt{2} \\
0 & 1.12 & -5.24 \\
\end{bmatrix}$
%\vspace{0.15cm}
\begin{center}
\begin{tikzpicture}[scale=0.6]
\draw[thick, ->] (0,0) -- (0, -4.3) node[midway,left] {$n$ rows};
\draw[thick, ->] (0.5,0.5) -- (6.1, 0.5) node[midway,above]{$m$ columns};
%\node[xshift=-0.6cm,yshift=-1.3cm] {Rows};
\node[gray,xshift=2.0cm,yshift=-1.3cm] {$\begin{bmatrix}
\Box & \Box & \Box & \ldots & \Box \\
\Box & \Box & \Box & \ldots & \Box \\
\Box & \textcolor{black}{a_{32}} & \Box & \ldots & \Box \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\Box & \Box & \Box & \ldots & \Box \\
\end{bmatrix}$};
\draw[draw=red] (0.5,-2.35) rectangle ++(5.6,0.8);
\draw[draw=blue] (1.6,-4.25) rectangle ++(1.0,4.25);
\draw [thick, blue, ->] (2.15,-4.4) to [out=-90,in=180] (3.0, -5.0) node[right] {\small{$2^{nd}$ column}};
\draw [thick, red, ->] (6.2,-1.95) to (7.,-1.95) node[right] {\small{$3^{rd}$ row}};
\end{tikzpicture}\hspace{0.5cm}
\end{center}
\item Consider a matrix $A$ with $n$ rows and $m$ columns.$ \begin{cases} \text{\textbf{Tall/Skinny}} & n > m\\
\text{\textbf{Square}} & n = m\\
\text{\textbf{Wide/Fat}} & n < m\\
\end{cases}$
\end{itemize}
\end{frame}

\begin{frame}[t]{Matrices}
\begin{itemize}
    \item $n$-vectors can be interpreted as $n \times 1$ matrices. These are called \textit{column vectors}.
    \item A matrix with only one row is called a \textit{row vector}, which can be referred to as $n$-row-vector.  $\mf{x} = \begin{bmatrix}1.45 & -3.1 & 12.4\end{bmatrix}$
    \item\textbf{Block matrices} \& \textbf{Submatrices}: $\mf{A} = \begin{bmatrix}
    \mf{B} & \mf{C} \\
    \mf{D} & \mf{E}
    \end{bmatrix}$.  What are the dimensions of the different matrices?
\end{itemize}
\end{frame}


\begin{frame}[t]{Matrices}
\begin{itemize}
    \item Matrices are also compact way to give a set of indexed column $n$-vectors, 
    $\mf{x}_1, \mf{x}_2, \mf{x}_3 \ldots \mf{x}_m$. 
    $$\mf{X} = \begin{bmatrix}
    \mf{x}_1 & \mf{x}_2 & \mf{x}_3 & \ldots & \mf{x}_m
    \end{bmatrix}$$

    \item \textbf{Zero matrix}$ = \mf{0}_{n\times m} = \begin{bmatrix}
    0 & 0 & \ldots & 0\\
    0 & 0 & \ldots & 0\\
    \vdots & \vdots & \ddots & \vdots\\
    0 & 0 & \ldots & 0
    \end{bmatrix}$

    \item \textbf{Identity matrix} is a square $n \times n$ matrix with all zero elements, except the diagonals where all elements are 1.
    $$i_{mn} = \begin{cases}
    1 & m = n\\
    0 & m \neq n
    \end{cases} \,\,\,\,\,\,\,\, \mf{I}_3=\begin{bmatrix}
    1 & 0 & 0\\
    0 & 1 & 0\\
    0 & 0 & 1\\
    \end{bmatrix} = \begin{bmatrix}
    \mf{e}_1 & \mf{e}_2 & \mf{e}_3
    \end{bmatrix}$$
\end{itemize}
\end{frame}


\begin{frame}[t]{Matrices}
\begin{itemize}

\item \textbf{Diagonal matrices} is a square matrix with non-zero elements on its diagonal.
$$\begin{bmatrix}
0.4 & 0 & 0 & 0\\
0 & -11 & 0 & 0\\
0 & 0 & 21 & 0\\
0 & 0 & 0 & 9.3\\
\end{bmatrix} = \text{\textbf{diag}}\left(0.4, -11, 21, 9.3\right)$$
\item \textbf{Triangular matrices}: Are square matrices. \textit{Upper triangular} $a_{ij} = 0, \forall i > j$; \textit{Lower triangular} $a_{ij} = 0, \forall i < j$.
\end{itemize}
\end{frame}

\begin{frame}[t]{Matrix operations: Transpose}
\begin{itemize}
\item \textbf{Transpose} switches the rows and columns of a matrix. $\mf{A}$ is a $n\times m$ matrix, then its transpose is represented by $\mf{A}^{\top}$, which is a $m \times n$ matrix.
\[ \mf{A} = \begin{bmatrix}
a_{11} & a_{12} & a_{13}\\
a_{21} & a_{22} & a_{23}\\
\end{bmatrix} \implies \mf{A}^{\top} = \begin{bmatrix}
a_{11} & a_{21}\\
a_{12} & a_{22}\\
a_{13} & a_{23}\\
\end{bmatrix} \]
Transpose converts between column and row vectors.\\
What is the transpose of a block matrix? $\mf{A} = \begin{bmatrix}
\mf{B} & \mf{C}\\\mf{D} & \mf{E}
\end{bmatrix}$
\end{itemize}
\end{frame}

\begin{frame}[t]{Matrix operations: Matrix Addition}
\begin{itemize}
\item \textbf{Matrix addition} can only be carried out with matrices of same size. Like vectors we perform element wise addition.
\[ \begin{bmatrix}
a_{11} & a_{12}\\
a_{21} & a_{22}\\
\end{bmatrix} + \begin{bmatrix}
b_{11} & b_{12}\\
b_{21} & b_{22}\\
\end{bmatrix} = \begin{bmatrix}
a_{11} + b_{11} & a_{12} + b_{12}\\
a_{21} + b_{21} & a_{22} + b_{22}\\
\end{bmatrix}\]

\item \textbf{Properties of matrix addition}:
\begin{itemize}
\item \textit{Commutative}: $\mf{A} + \mf{B} = \mf{B} + \mf{A}$
\item \textit{Associative}: $\left(\mf{A} + \mf{B}\right) + \mf{C} = \mf{A} + \left(\mf{B} + \mf{C}\right)$
\item \textit{Addition with zero matrix}: $\mf{A} + \mf{0} = \mf{0} + \mf{A} = \mf{A}$
\item \textit{Transpose of sum}: $\left(\mf{A} + \mf{B}\right)^{\top} = \mf{A}^{\top} + \mf{B}^{\top}$
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[t]{Matrix operations: Scalar multiplication}
\begin{itemize}
\item \textbf{Scalar multiplication} Each element of the matrix gets multiplied by the scalar.
\[ \alpha \begin{bmatrix}
a_{11} & a_{12}\\
a_{21} & a_{22}\\
\end{bmatrix} = \begin{bmatrix}
\alpha a_{11} & \alpha a_{12} \\
\alpha a_{21} & \alpha a_{22} \\
\end{bmatrix}\]
\item We will mostly only deal with matrices with real entries. Such matrices are elements of the set $\mathbb{R}^{n\times m}$.
\item Given the aforementioned matrix operations and their properties, is $\mathbb{R}^{n\times m}$ a vector space?
\end{itemize}
\end{frame}

\begin{frame}[t]{Matrix operations: Matrix multiplication}
\begin{itemize}
\item A useful multiplication operation can be defined for matrices.
\item It is possible to \textit{multiply} two matrices $\mf{A} \in \mathbb{R}^{n\times p}$ and $\mf{B} \in \mathbb{R}^{p\times m}$ through this \textit{matrix multiplication} procedure.
\item The product matrix $\mf{C} \coloneqq \mf{A}\mf{B} \in \mathbb{R}^{n \times m}$, if the number of columns of $\mf{A}$ is equal to the number of rows of $\mf{B}$.
\[ c_{ij} \coloneqq \sum_{k=1}^{p} a_{ik}b_{kj} \,\,\,\,\, \forall i \in \left\{1, \ldots n\right\}\,\,\, , j \in \left\{1 \ldots m\right\} \]
\end{itemize}
\end{frame}

\begin{frame}[t]{Matrix multiplication}
\begin{itemize}
\item \textit{Inner product} is a special case of matrix multiplication between a \textit{row vector} and a \textit{column vector}.
\[ \mf{x}^{\top}\mf{y} = \begin{bmatrix}
x_1 \\ x_2 \\ \vdots \\x_n
\end{bmatrix}^{\top}\begin{bmatrix}
y_1 \\ y_2 \\ \vdots \\y_n
\end{bmatrix} = \begin{bmatrix}
x_1 & x_2 & \ldots &x_n
\end{bmatrix}\begin{bmatrix}
y_1 \\ y_2 \\ \vdots \\y_n
\end{bmatrix} = \sum_{i=1}^nx_iy_i\]
\end{itemize}
\end{frame}

\begin{frame}[t]{Matrix multiplication: Post-multiplication by a column vector}
\begin{itemize}
\item Consider a matrix $\mf{A} \in \mathbb{R}^{n \times m}$ and a $m$-vector $\mf{x} \in \mathbb{R}^m$. We can multiply $\mf{A}$ and $\mf{x}$ to obtain $\mf{y} = \mf{A}\mf{x} \in \mathbb{R}^n$.
\[ \mf{y} = \begin{bmatrix}
a_{11} & a_{12} & \ldots & a_{1m} \\
a_{21} & a_{22} & \ldots & a_{2m} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n1} & a_{n2} & \ldots & a_{nm} \\
\end{bmatrix}\begin{bmatrix}
x_1\\ x_2 \\ \vdots \\ x_{m}
\end{bmatrix} = \begin{bmatrix}
\sum_{i=1}^ma_{1i}x_i\\ \sum_{i=1}^ma_{2i}x_i \\ \vdots \\ \sum_{i=1}^ma_{ni}x_i
\end{bmatrix} = \sum_{i=1}^mx_i\begin{bmatrix}
a_{1i} \\ a_{2i} \\ \vdots \\ a_{ni}
\end{bmatrix} = \sum_{i=1}^mx_i \mathbf{a}_{i} \]
\item Post-multiplying a matrix $\mf{A}$ by a column vector $\mf{x}$ results in a linear combination of the columns of matrix $\mf{A}$.

\item $\mathbf{x}$ provides the column mixture.
\end{itemize}
\end{frame}

\begin{frame}[t]{Matrix multiplication: Pre-multiplication by a row vector}
\begin{itemize}
\item Let $\mf{x}^{\top} \in \mathbb{R}^{n}$ and $\mf{A} \in \mathbb{R}^{n \times m}$, then $\mf{y} = \mf{x}^{\top}\mf{A}$.
\[ \mf{y} = \begin{bmatrix}
x_1 & \ldots & x_n
\end{bmatrix} \begin{bmatrix}
a_{11} & \ldots & a_{1m} \\
\vdots & \ddots & \vdots \\
a_{n1} & \ldots & a_{nm} \\
\end{bmatrix} = \begin{bmatrix} \sum_{i=1}^{n} x_i a_{i1} & \ldots & \sum_{i=1}^{n} x_i a_{im}
\end{bmatrix} = \sum_{i=1}^n x_i \tilde{\mf{a}}_i^\top \]
where, $\tilde{\mf{a}}_i^\top = \begin{bmatrix}
a_{i1} & \ldots & a_{im}\end{bmatrix}$
\item Pre-multiplying a matrix $\mf{A}$ by a row vector $\mf{x}$ results in a linear combination of the rows of $\mf{A}$.

\item $\mf{x}^\top$ provides the row mixture.
\end{itemize}
\end{frame}

\begin{frame}[t]{Matrix multiplication}
\begin{itemize}
\item Multiplying two matrices $\mf{A} \in \mathbb{R}^{n \times p}$ and $\mf{B} \in \mathbb{R}^{p \times m}$ produces $\mf{C} \in \mathbb{R}^{n \times m}$,
\[ \mf{C} = \mf{A}\mf{B} = \begin{bmatrix}
a_{11} & a_{12} & \ldots & a_{1p} \\
a_{21} & a_{22} & \ldots & a_{2p} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n1} & a_{n2} & \ldots & a_{np} \\
\end{bmatrix} \begin{bmatrix}
b_{11} & b_{12} & \ldots & b_{1m} \\
b_{21} & b_{22} & \ldots & b_{2m} \\
\vdots & \vdots & \ddots & \vdots \\
b_{p1} & b_{p2} & \ldots & b_{pm} \\
\end{bmatrix} = \begin{bmatrix}
c_{11} & c_{12} & \ldots & c_{1m} \\
c_{21} & c_{22} & \ldots & c_{2m} \\
\vdots & \vdots & \ddots & \vdots \\
c_{p1} & c_{n2} & \ldots & c_{nm} \\
\end{bmatrix}\]

\item \textbf{Four interpretations of matrix multiplication.}
\begin{enumerate}
  \item Inner-Product interpretation
  \item Column interpretation
  \item Row interpretation
  \item Outer product interpretation.
\end{enumerate} 
\end{itemize}
\end{frame}

\begin{frame}[t]{Matrix multiplication: Inner-product Interpreation}
\[ \mf{C} = \mf{A}\mf{B}, \,\,\, \mf{A} \in \mathbb{R}^{n \times p}, \, \mf{B} \in \mathbb{R}^{p \times m}, \, \mf{C} \in \mathbb{R}^{n \times m} \]
\begin{itemize}
\item $ij^{th}$ element of $\mf{C}$ is the inner product of the $i^{th}$ row of $\mf{A}$ and the $j^{th}$ column of $\mf{B}$.
 \[ c_{ij} = \sum_{k=1}^{m} a_{ik} b_{kj} = \tilde{\mf{a}}_i^{\top}\mf{b}_j \]
 where, $i \in \left\{1 \ldots n\right\}, j \in \left\{1 \ldots m\right\}$
\end{itemize}
\end{frame}


\begin{frame}[t]{Matrix multiplication: Column interpretation}
\[ \mf{C} = \mf{A}\mf{B}, \,\,\, \mf{A} \in \mathbb{R}^{n \times p}, \, \mf{B} \in \mathbb{R}^{p \times m}, \, \mf{C} \in \mathbb{R}^{n \times m} \]
\begin{itemize}
\item Columns of $\mf{C}$ are the linear combinations of the columns of $\mf{A}$.
\[ \mf{C} = \mf{A} \begin{bmatrix}
\mf{b}_{1} & \mf{b}_{2} & \ldots & \mf{b}_{m}
\end{bmatrix} = \begin{bmatrix}
\mf{A}\mf{b}_{1} & \mf{A}\mf{b}_{2} & \ldots & \mf{A}\mf{b}_{m}
\end{bmatrix} \]

\item $j^{th}$ column of $\mf{C}$ is the linear combination of the columns of $\mf{A}$
\[ \mf{c}_j = \sum_{k=1}^{p} b_{kj} \mf{a}_k \]
\end{itemize}
\end{frame}


\begin{frame}[t]{Matrix multiplication: Row interpretation}
\[ \mf{C} = \mf{A}\mf{B}, \,\,\, \mf{A} \in \mathbb{R}^{n \times p}, \, \mf{B} \in \mathbb{R}^{p \times m}, \, \mf{C} \in \mathbb{R}^{n \times m} \]
\begin{itemize}
\item Rows of $\mf{C}$ are the linear combinations of the rows of $\mf{B}$.
\[ \mf{C} = \begin{bmatrix}
\tilde{\mf{a}}_{1}^\top \\ \tilde{\mf{a}}_{2}^\top \\ \ldots \\ \tilde{\mf{a}}_{n}^\top
\end{bmatrix} \mf{B}  = \begin{bmatrix}
\tilde{\mf{a}}_{1}^\top \mf{B} \\ \tilde{\mf{a}}_{2}^\top \mf{B} \\ \ldots \\ \tilde{\mf{a}}_{n}^\top \mf{B}
\end{bmatrix} \]

\item $i^{th}$ row of $\mf{C}$ is the linear combination of the rows of $\mf{B}$
\[ \tilde{\mf{c}}_i^\top = \sum_{k=1}^{p} a_{ik} \tilde{\mf{b}}_{k}^\top  \]
\end{itemize}
\end{frame}


\begin{frame}[t]{Matrix multiplication: Outer product interpretation}
\begin{itemize}
    \item \textbf{Outer product}: Product between a colum vector and a row vector. Let $\mf{x} \in \mathbb{R}^n$ and $\mf{y} \in \mathbb{R}^m$. The \textit{outer product} is defined as,
    \[ \mf{x}\mf{y}^{\top} = \begin{bmatrix}
    x_1\\ x_2\\ \vdots \\x_n
    \end{bmatrix} \begin{bmatrix}
    y_1 &  y_2 & \ldots & y_m
    \end{bmatrix} = \begin{bmatrix}
    x_1y_1 &  x_1y_2 & \ldots & x_1y_m \\
    x_2y_1 &  x_2y_2 & \ldots & x_2y_m \\
    \vdots &  \vdots & \ddots & \vdots \\
    x_ny_1 &  x_ny_2 & \ldots & x_ny_m \\
    \end{bmatrix} \in \mb{R}^{n \times m} \]
\end{itemize}
\end{frame}


\begin{frame}[t]{Matrix multiplication: Outer product interpretation}
\[ \mf{C} = \mf{A}\mf{B}, \,\,\, \mf{A} \in \mathbb{R}^{n \times p}, \, \mf{B} \in \mathbb{R}^{p \times m}, \, \mf{C} \in \mathbb{R}^{n \times m} \]
\begin{itemize}
    \item $\mf{C}$ can be written as the sum of $p$ outer products of columns of $\mf{A}$ and rows of $\mf{B}$.
    \[ \mf{C} = \mf{A}\mf{B} = \begin{bmatrix}
    \mf{a}_1 & \mf{a}_2 & \mf{a}_3 & \ldots & \mf{a}_p
    \end{bmatrix} \begin{bmatrix}
    \tilde{\mf{b}}_1^{\top} \\
    \tilde{\mf{b}}_2^{\top} \\
    \tilde{\mf{b}}_3^{\top} \\
    \vdots \\
    \tilde{\mf{b}}_p^{\top}
    \end{bmatrix} = \sum_{i=1}^{p}\mf{a}_i\tilde{\mf{b}}_i^{\top} \]
\end{itemize}
\end{frame}


\begin{frame}[t]{Properties of matrix multiplication}
\begin{itemize}
\item \textbf{Not commutative}: $\mf{A}\mf{B} \neq \mf{B}\mf{A}$\\
The product of two matrices might not alwasys be defined. When it is defined, $\mf{A}\mf{B}$ and $\mf{B}\mf{A}$ need not match.
\item \textbf{Distributive}:  $\mf{A}\left(\mf{B} + \mf{C}\right) = \mf{A}\mf{B} + \mf{B}\mf{C}$ and $\left(\mf{A} + \mf{B}\right)\mf{C} = \mf{A}\mf{C} + \mf{B}\mf{C}$ 
\item \textbf{Associative}: $\mf{A}\left(\mf{B}\mf{C}\right) = \left(\mf{A}\mf{B}\right)\mf{C} $
\item \textbf{Transpose}: $\left(\mf{A}\mf{B}\right)^{\top} = \mf{B}^{\top}\mf{A}^{\top}$
\item \textbf{Scalar product}: $\alpha\left(\mf{A}\mf{B}\right) = \left(\alpha \mf{A}\right)\mf{B} = \mf{A}\left(\alpha \mf{B}\right)$
\end{itemize}
\end{frame}

\begin{frame}[t]{Linear equations}
\begin{itemize}
\item Matrices present a compact way to represent a set of linear equations. Consider the following,
\[
\begin{rcases*}
\begin{split}
a_{11}x_1 + a_{12}x_2 \ldots + a_{1m}x_m & = b_1 \\
a_{21}x_1 + a_{22}x_2 \ldots + a_{2m}x_m & = b_2 \\
a_{31}x_1 + a_{32}x_2 \ldots + a_{3m}x_m & = b_3 \\
\vdots \\
a_{n1}x_1 + a_{n2}x_2 \ldots + a_{nm}x_m & = b_n \\
\end{split}
\end{rcases*} \, \longrightarrow \mf{Ax} = \mf{b}, \,\,\, \mf{A} \in \mathbb{R}^{n \times m}, \,\, \mf{x} \in \mathbb{R}^n, \,\, \mf{b} \in \mathbb{R}^m
\]

\[
\mf{A} = \begin{bmatrix}
a_{11} & a_{12} & a_{13} & \ldots & a_{1m}\\
a_{21} & a_{22} & a_{23} & \ldots & a_{2m}\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
a_{n1} & a_{n2} & a_{n3} & \ldots & a_{nm}\\
\end{bmatrix} \,\,\,\,\, 
\mf{x} = \begin{bmatrix}
x_1\\ x_2\\ \vdots \\ x_m
\end{bmatrix} \,\,\,\,\,
\mf{b} = \begin{bmatrix}
b_1\\ b_2\\ \vdots \\ b_n
\end{bmatrix}
\]
\end{itemize}
\end{frame}


\begin{frame}[t]{Linear equations in control problems}
\begin{LARGE}
\[ \mf{x}: \text{Input} \,\,\, \mf{b}: \text{Output} \,\,\, \mf{A}: \text{System dynamics} \]

\[ \begin{bmatrix}
b_1\\ b_2\\ \vdots \\ b_n
\end{bmatrix} = \begin{bmatrix}
a_{11} & a_{12} & a_{13} & \ldots & a_{1m}\\
a_{21} & a_{22} & a_{23} & \ldots & a_{2m}\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
a_{n1} & a_{n2} & a_{n3} & \ldots & a_{nm}\\
\end{bmatrix} \,\, \begin{bmatrix}
x_1\\ x_2\\ \vdots \\ x_m
\end{bmatrix} \]
\end{LARGE}
\end{frame}

\begin{frame}
\end{frame}


\begin{frame}[t]{Linear equations in estimation problems}
\begin{LARGE}
\[ \mf{x}: \text{Parameter} \,\,\, \mf{b}: \text{Measurements} \,\,\, \mf{A}: \text{System characteristics} \]

\[ \begin{bmatrix}
b_1\\ b_2\\ \vdots \\ b_n
\end{bmatrix} = \begin{bmatrix}
a_{11} & a_{12} & a_{13} & \ldots & a_{1m}\\
a_{21} & a_{22} & a_{23} & \ldots & a_{2m}\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
a_{n1} & a_{n2} & a_{n3} & \ldots & a_{nm}\\
\end{bmatrix} \,\, \begin{bmatrix}
x_1\\ x_2\\ \vdots \\ x_m
\end{bmatrix} \]
\end{LARGE}
\end{frame}


\begin{frame}
\end{frame}


\begin{frame}[t]{Rank of a matrix $\mf{A}$}
\begin{itemize}
\item \textbf{Rank of a matrix $\mf{A}$}: dimension of the subspace spanned by the columns of $\mf{A}$ or the rows of $\mf{A} \in \mb{R}^{n \times m}$. 
\[ \begin{split} 
rank\lp \mf{A} \rp &= \mathrm{dim} \, span\lp \lc \mf{a}_1, \mf{a}_2, \ldots \mf{a}_m \rc \rp \rightarrow \text{Column rank}\\ 
&= \mathrm{dim} \, span\lp \lc \tilde{\mf{a}}_1^\top, \tilde{\mf{a}}_2^\top, \ldots \tilde{\mf{a}}_n^\top \rc \rp \rightarrow \text{Row rank}
\end{split}
\]

\item Column Rank is always equal to the row rank.

\item Rank tells us the number of independent columns/row in the matrix.

\item \textbf{Full rank matrix } $\mf{A}$: $rank\lp \mf{A} \rp  = \min\lp n, m\rp$\\
      \textbf{Rank deficient matrix } $\mf{A}$: $rank\lp \mf{A} \rp  < \min\lp n, m\rp$
\end{itemize}
\end{frame}


\begin{frame}[t]{Geometry of linear equations}
\vspace{-0.5cm}
\[\begin{rcases*}
\begin{split} x_1 + 2x_2 &= -1 \\ x_1 + x_2 &= 1 \end{split}
\end{rcases*} \longrightarrow \bmx
1 & 2\\
1 & 1
\emx \bmx
x_1\\
x_2
\emx = \bmx
-1\\
1
\emx\]
Two ways to view this: \textbf{row view} and the \textbf{column view}.
\begin{center}
\begin{tikzpicture}[scale=0.5]
\node(0, 0) [yshift=2.4cm] {\textbf{Traditional (row) view}};
\node(0, 0) [xshift=1.6cm,yshift=1.7cm] {\Large {$\mb{R}^m$}};
\draw[gray,<->] (-4, 0) -- (4, 0) node[right,below] {$x1$};
\draw[gray,<->] (0, -4) -- (0, 4) node[right] {$x2$};
\draw[blue,thick] (-4,3/2) -- (4, -5/2) node[midway,left,yshift=-0.25cm] {$x_1 + 2x_2 = -1$};
\draw[red,thick] (-3,4) -- (4, -3) node[midway,right,yshift=0.25cm] {$x_1 + x_2 = 1$};
\draw (3,-2) node[] {$\bullet$};
\draw (3,-2) node[right,yshift=0.25cm] {$\left(3,-2\right)$};
\end{tikzpicture}\hspace{1cm}
\begin{tikzpicture}[scale=0.5]
\node(0, 0) [yshift=2.4cm] {\textbf{Column view}};
\node(0, 0) [xshift=1.6cm,yshift=2.0cm] {\Large {$\mb{R}^n$}};
\draw[gray,<->] (-4, 0) -- (4, 0);
\draw[gray,<->] (0, -4) -- (0, 4);
\draw[gray,thin,->] (0,0) -- (3,3) node[above,right] {$3\mf{a}_1$};
\draw[blue,thick,->] (0,0) -- (1,1) node[above,yshift=0.cm] {\small{$\mf{a}_1$}};
\draw[red,thick,->] (0,0) -- (2,1) node[above,xshift=0.1cm,yshift=0.cm] {\small{$\mf{a}_2$}};
\draw[gray,thin,->] (0,0) -- (-4,-2) node[below,left] {$-2\mf{a}_2$};
\draw[gray,thin,dashed] (3,3) -- (-1,1);
\draw[gray,thin,dashed] (-4,-2) -- (-1,1);
\draw[black!30!green,thick,->] (0,0) -- (-1,1) node[above,left] {\small{$\mf{b}$}};
\end{tikzpicture}
\end{center}
\end{frame}

\begin{frame}[t]{Solutions of linear equations}
\vspace{-0.5cm}
$$ \mf{Ax = b}, \,\,\, \mf{A} \in \mathbb{R}^{m \times n}, \,\, \mf{x} \in \mathbb{R}^n, \,\, \mf{b} \in \mathbb{R}^m$$
\vspace{-0.5cm}
\begin{itemize}
\item \textbf{Three possible situations:} \textsc{No solution}, \textsc{Infinitely many solutions}, or \textsc{Unique Solution}.
\item When do have infinitely many or no solutions? In $\mathbb{R}^3$, we can visualize the different situations.
\end{itemize}
\begin{center}
\begin{tikzpicture}[scale=0.5]
\node[yshift=1.5cm] {\small{Two parallel planes}};
\draw[black,-] (-2, -2) -- (2, 2);
\draw[black,-] (-3, -2) -- (1, 2);
\draw[black,-] (2, -2) -- (-2, 2);
\end{tikzpicture}\hspace{0.25cm}
\begin{tikzpicture}[scale=0.5]
\node[yshift=1.5cm] {\small{Three parallel planes}};
\draw[black,-] (-2, -2) -- (2, 2);
\draw[black,-] (-3, -2) -- (1, 2);
\draw[black,-] (-1, -2) -- (3, 2);
\end{tikzpicture}\hspace{0.25cm}
\begin{tikzpicture}[scale=0.5]
\node[yshift=1.0cm] {\small{No intersection}};
\draw[black,-] (-3, -2) -- (3, -2);
\draw[black,-] (-3, -3) -- (0, 1);
\draw[black,-] (3, -3) -- (-1, 1);
\end{tikzpicture}\hspace{0.25cm}
\begin{tikzpicture}[scale=0.5]
\node[yshift=1.5cm] {\small{Line intersection}};
\draw[black,-] (-2, 0) -- (2, 0);
\draw[black,-] (-2, -2) -- (2, 2);
\draw[black,-] (2, -2) -- (-2, 2);
\end{tikzpicture}
\end{center} 
\end{frame}


\begin{frame}[t]{Understanding $\mf{A}\mf{x} = \mf{b}$: Unique solution}

\begin{columns}[t]
\begin{column}{0.3\textwidth}
\[ \bmx
1 & 2\\
1 & 1
\emx\bmx
x_1\\
x_2
\emx = \bmx
-1\\
1
\emx \]
\begin{center}
\begin{tikzpicture}[scale=0.5]
\draw[gray,<->] (-4, 0) -- (4, 0);
\draw[gray,<->] (0, -4) -- (0, 4);
\draw[gray,thin,->] (0,0) -- (3,3) node[above,right] {$3\mf{a}_1$};
\draw[blue,thick,->] (0,0) -- (1,1) node[above,yshift=0.cm] {\small{$\mf{a}_1$}};
\draw[red,thick,->] (0,0) -- (2,1) node[above,xshift=0.1cm,yshift=0.cm] {\small{$\mf{a}_2$}};
\draw[gray,thin,->] (0,0) -- (-4,-2) node[below,left] {$-2\mf{a}_2$};
\draw[gray,thin,dashed] (3,3) -- (-1,1);
\draw[gray,thin,dashed] (-4,-2) -- (-1,1);
\draw[black!30!green,thick,->] (0,0) -- (-1,1) node[above,left] {\small{$\mf{b}$}};
\end{tikzpicture}
\end{center}
\end{column}
\hspace{0.5cm}
\begin{column}{0.6\textwidth}
\begin{itemize}
  \item Square matrix
  \item Linearly independent set of columns $\lc \mf{a}_1, \mf{a}_2 \rc$
  \item $\mf{b} \in span\lp{\lc \mf{a}_1, \mf{a}_2 \rc}\rp$.
  \item Always solvable, and give an unique solution.
\end{itemize}
\end{column}
\end{columns}
\end{frame}


\begin{frame}[t]{Understanding $\mf{A}\mf{x} = \mf{b}$: Unique solution or No solution}
\begin{columns}[t]
\begin{column}{0.3\textwidth}
\begin{enumerate}
  \item $\bmx 2\\ 1 \emx \bmx x_1 \emx = \mf{b}_1 = \bmx -1\\ 1 \emx$
  \item $\bmx 2\\ 1 \emx \bmx x_1 \emx = \mf{b}_2 = \bmx 3\\ 1.5 \emx$
\end{enumerate}

\begin{center}
\begin{tikzpicture}[scale=0.5]
\draw[gray,<->] (-4, 0) -- (4, 0);
\draw[gray,<->] (0, -4) -- (0, 4);
\draw[black,dashed] (4, 2) -- (-4,-2) node[xshift=1cm,,yshift=-0.2cm] {\small{$span\lp \lc \mf{a}_1 \rc\rp$}};
\draw[black!30!green,thick,->] (0,0) -- (3,1.5) node[above,xshift=0.0cm,yshift=0.cm] {\small{$\mf{b}_2$}};
\draw[blue,thick,->] (0,0) -- (2,1) node[above,xshift=-0.2cm,yshift=0.cm] {\small{$\mf{a}_1$}};
% \draw[gray,thin,dashed] (3,3) -- (-1,1);
% \draw[gray,thin,dashed] (-4,-2) -- (-1,1);
\draw[black!5!red,thick,->] (0,0) -- (-1,1) node[above,left] {\small{$\mf{b}_1$}};
\end{tikzpicture}
\end{center}
\end{column}
\hspace{0.5cm}
\begin{column}{0.6\textwidth}
\begin{itemize}
  \item Tall matrix
  \item Linearly independent set of columns $\lc \mf{a}_1 \rc$
\end{itemize}
\vspace{0.25cm}
$\mf{b}_1 \notin span\lp{\lc \mf{a}_1 \rc}\rp \implies$ Not solvable.

\vspace{0.25cm}
$\mf{b}_2 \in span\lp{\lc \mf{a}_1 \rc}\rp \implies$ Solvable with unqiue solution.
\end{column}
\end{columns}
\end{frame}


\begin{frame}[t]{Understanding $\mf{A}\mf{x} = \mf{b}$: Infinitely many solution}
\begin{columns}[t]
\begin{column}{0.3\textwidth}
\[ \bmx
1 & 2 & -1\\
1 & 1 & -2
\emx\bmx
x_1\\
x_2\\
x_3
\emx = \bmx
-1\\
1
\emx \]
\begin{center}
\begin{tikzpicture}[scale=0.5]
\draw[gray,<->] (-4, 0) -- (4, 0);
\draw[gray,<->] (0, -4) -- (0, 4);
% \draw[gray,thin,->] (0,0) -- (3,3) node[above,right] {$3a_1$};
\draw[blue,thick,->] (0,0) -- (1,1) node[above,yshift=0.cm] {\small{$\mf{a}_1$}};
\draw[red,thick,->] (0,0) -- (2,1) node[above,xshift=0.1cm,yshift=0.cm] {\small{$\mf{a}_2$}};
\draw[black!30!green,thick,->] (0,0) -- (-1,-2) node[above,xshift=0cm,yshift=-0.4cm] {\small{$\mf{a}_3$}};
\draw[black,thick,->] (0,0) -- (-1,1) node[above,left] {\small{$b$}};
\end{tikzpicture}
\end{center}
\end{column}
\hspace{0.5cm}
\begin{column}{0.6\textwidth}
\begin{itemize}
  \item Fat matrix
  \item Linearly dependent set of columns $\lc \mf{a}_1, \mf{a}_2, \mf{a}_3 \rc$
  \item $\mf{b} \in span\lp{\lc \mf{a}_1, \mf{a}_2, \mf{a}_3 \rc}\rp$.
  \item Always solvable, with infinitely many solutions.
\end{itemize}
\end{column}
\end{columns}
\end{frame}


\begin{frame}[t]{Understanding $\mf{A}\mf{x} = \mf{b}$: Conditions for different types of solutions}
\[ \mf{A}\mf{x} = \mf{b}, \,\,\, \mf{A} \in \mb{R}^{n \times m}, \, \mf{x} = \mb{R}^m, \, \mf{b} \in \mb{R}^n \]

\textbf{Full rank } $\mf{A}$:
\begin{itemize}
  \item $rank\lp \mf{A} \rp = n \implies$ \textbf{Always solvable}\\
        \vspace{0.2cm}
        $\begin{cases} 
        n = m & \implies \text{Unique solution} \\ 
        n < m & \implies \text{Infinitely many solutions} \\ 
        \end{cases}$
  \item $rank\lp \mf{A} \rp = m  \implies$ \textbf{No infinite solutions}\\
        \vspace{0.2cm}
        $\begin{cases} 
        m = n & \implies \text{Unique solution} \\ 
        m < n & \rightarrow \begin{cases}
        \mf{b} \in span\lp \mf{a}_1, \ldots \mf{a}_m \rp {\implies} \text{Unique solution} \\
        \mf{b} \notin span\lp \mf{a}_1, \ldots \mf{a}_m \rp {\implies} \text{No solution}
        \end{cases}\\ 
        \end{cases}$
\end{itemize}
\end{frame}


\begin{frame}[t]{Understanding $\mf{A}\mf{x} = \mf{b}$: Conditions for different types of solutions}
\[ \mf{A}\mf{x} = \mf{b}, \,\,\, \mf{A} \in \mb{R}^{n \times m}, \, \mf{x} = \mb{R}^m, \, \mf{b} \in \mb{R}^n \]

\textbf{Rank deficient} $\mf{A}$:
\begin{itemize}
  \item $rank\lp \mf{A} \rp < \min \lp n, m \rp \implies$ \textbf{No unique solution}\\
        \vspace{0.2cm}
        $\begin{cases}
        \mf{b} \in span\lp \mf{a}_1, \ldots \mf{a}_m \rp {\implies} \text{Infinitely many solutions} \\
        \mf{b} \notin span\lp \mf{a}_1, \ldots \mf{a}_m \rp {\implies} \text{No solution}
        \end{cases}$
\end{itemize}
\end{frame}


\begin{frame}[t]{Understanding $\mf{A}\mf{x} = \mf{b}$: Conditions for different types of solutions}
\[ \mf{A}\mf{x} = \mf{b}, \,\,\, \mf{A} \in \mb{R}^{n \times m}, \, \mf{x} = \mb{R}^m, \, \mf{b} \in \mb{R}^n \]
\vspace{0.5cm}
\begin{itemize}
  \item $\mf{b} \notin span\lp \mf{a}_1, \ldots \mf{a}_m \rp {\implies} \text{No solution}$
  \vspace{-2cm}
  \item $\mf{b} \in span\lp \mf{a}_1, \ldots \mf{a}_m \rp {\implies} \begin{cases}
        rank\lp \mf{A} \rp = m \implies \text{Unique} \\
        rank\lp \mf{A} \rp < m \implies \text{Infinitely many solutions}
        \end{cases}$
\end{itemize}
\end{frame}

\begin{frame}[t]{General solution of linear equations}
\[ \mf{A}\mf{x} = \mf{b}, \,\,\, \mf{A} \in \mb{R}^{n \times m}, \, \mf{x} = \mb{R}^m, \, \mf{b} \in \mb{R}^n \]

\begin{itemize}
  \item Assuming that this system can be solved, the most general form of the solution is,
  \[ \mf{x} = \mf{x}_p + \mf{x}_h \]
  where, $\mf{x}_p$ is called the particular solution, and $\mf{x}_h$ is the homogenous solution.

  \item \textbf{Homogenous solution}: Solution of the equation $\mf{A}\mf{x} = \mf{0}$.

  \item The set of all homogenous solutions of $\mf{A}$ -- $\lc \mf{x}_h \, \vert \, \mf{A}\mf{x}_h = \mf{0} \rc$ -- form a subspace of $\mb{R}^m$.
\end{itemize}
\end{frame}

\begin{frame}[t]{Geometry of the general solution}
\[ \mf{A}\mf{x} = \mf{b}, \,\,\, \mf{A} \in \mb{R}^{n \times m}, \, \mf{x} = \mb{R}^m, \, \mf{b} \in \mb{R}^n \]
\begin{columns}[t]
\begin{column}{0.3\textwidth}
\[ \mf{A} = \bmx
1 & -1\\
2 & -2
\emx, \,\, \mf{b} = \bmx
1.5\\
3
\emx \]
\end{column}
\hspace{0.5cm}
\begin{column}{0.6\textwidth}
\end{column}
\end{columns}
\end{frame}

\begin{frame}[t]{Geometry of the general solution}
\begin{center}
\vspace{-0.2cm}
\begin{small}
$\mf{A} = \bmx
1 &  2\\
1 &  1
\emx, \,\, \mf{b} = \bmx
-1\\
1
\emx$
\end{small}
\vspace{0.5cm}

\begin{tikzpicture}[scale=0.45]
\node[] at (-5, 5) {\Large $\mb{R}^m$};
\draw[gray,<->] (-6, 0) -- (6, 0) node[right,below] {$x_1$};
\draw[gray,<->] (0, -6) -- (0, 6) node[right] {$x_2$};
\draw[black,thick,->] (0,0) -- (3,-2);
\node[above] at (3, -2) {$\mf{x}_p$};
\node[black!30!green,above left] at (0, 0) {\small $\mf{x}_h = \mf{0}$};

\node[] at (-5 + 14, 5) {\Large $\mb{R}^n$};
\draw[gray,<->] (-6 + 14, 0) -- (6 + 14, 0);
\draw[gray,<->] (0 + 14, -6) -- (0 + 14, 6);
% \node[above, xshift=-0.5cm]  at (-3.2 + 14, -4) {\small $span\lp \lc \mf{a}_1, \mf{a}_2 \rc \rp$};
% \draw[black!30!green,thick,->] (0 + 14,0) -- (2 + 14,4) node[above,right] {$\mf{b}$};
\draw[red,thick,->] (0 + 14,0) -- (1 + 14,1) node[xshift=-0.1cm,above] {\small $\mf{a}_1$};
\draw[blue,thick,->] (0 + 14,0) -- (2 + 14,1) node[below,yshift=-0.05cm] {\small{$\mf{a}_2$}};
\draw[red,thin,dashed] (-6 + 14,-6) -- (6 + 14,6);
\node[red, rotate=45] at (3 + 14, 3.9) {\small $span\lp\lc \mf{a}_1 \rc\rp$};
\draw[blue,thin,dashed] (-6 + 14,-3) -- (6 + 14,3);
\node[blue, rotate=26.6] at (4.5 + 14, 1.6) {\small $span\lp\lc \mf{a}_2 \rc\rp$};
\draw[black!30!green,thick,->] (0 + 14,0) -- (-1 + 14,1);
\node[above left] at (-1, 1) {\small $\mf{b}$};
\draw[red, thin, -latex] (3+0.1, -2) to[out=30,in=-135] (-1 + 14 - 0.1, 1);
\node[] at (7, -1.5) {$\mf{A}$};
\end{tikzpicture}
\end{center}
\end{frame}


\begin{frame}[t]{Geometry of the general solution}
\begin{center}
\vspace{-0.2cm}
\begin{small}
$\mf{A} = \bmx
2\\
1
\emx, \,\, \mf{b} = \bmx
-4\\
-2
\emx$
\end{small}
\vspace{0.5cm}

\begin{tikzpicture}[scale=0.45]
\node[] at (-5, 5) {\Large $\mb{R}^m$};
\draw[gray,<->] (-6, 0) -- (6, 0) node[right,below] {$x_1$};
\draw[black,thick,->] (0,0) -- (-2,0);
\node[above left] at (-2, 0) {$\mf{x}_p$};
\node[black!30!green,below] at (0, 0) {\small $\mf{x}_h = \mf{0}$};

\node[] at (-5 + 14, 5) {\Large $\mb{R}^n$};
\draw[gray,<->] (-6 + 14, 0) -- (6 + 14, 0);
\draw[gray,<->] (0 + 14, -6) -- (0 + 14, 6);
\draw[blue,thick,->] (0 + 14,0) -- (2 + 14,1) node[below,yshift=-0.05cm] {\small{$\mf{a}_1$}};
\draw[blue,thin,dashed] (-6 + 14,-3) -- (6 + 14,3);
\node[blue, rotate=26.6] at (4.5 + 14, 1.6) {\small $span\lp\lc \mf{a}_1 \rc\rp$};
\draw[black!30!green,thick,->] (0 + 14,0) -- (-4 + 14,-2);
\node[above left] at (-4 + 14, -2) {\small $\mf{b}$};
\draw[red, thin, -latex] (-2, 0 - 0.1) to[out=-90,in=-100] (-4 + 14, -2);
\node[] at (5, -4) {$\mf{A}$};
\end{tikzpicture}
\end{center}
\end{frame}


\begin{frame}[t]{Geometry of the general solution}
\begin{center}
\vspace{-0.2cm}
\begin{small}
$\mf{A} = \bmx
2 &  1
\emx, \,\, \mf{b} = \bmx
5
\emx$
\end{small}
\vspace{0.5cm}


\begin{tikzpicture}[scale=0.45]
\node[] at (-5, 5) {\Large $\mb{R}^m$};
\draw[gray,<->] (-6, 0) -- (6, 0) node[right,below] {$x_1$};
\draw[gray,<->] (0, -6) -- (0, 6) node[right] {$x_2$};
\draw[black,thick,->] (0,0) -- (2,1);
\node[] at (1, 1.2) {$\mf{x}_p$};
\draw[black!30!green,thick,dotted] (-3, 6) -- (3,-6);
\node[black!30!green,below,rotate=-63.43] at (3, -3) {\small $\lc \mf{x}_h \, \vert \, \mf{A}\mf{x}_h = \mf{0} \rc$};
\draw[black!30!green,thick] (5.5,-6) -- (-0.5,6);
\node[black!30!green,rotate=-63.43] at (1.2, 4) {\small $\mf{x}_p + \mf{x}_h$};

\node[] at (5 + 14, 5) {\Large $\mb{R}^n$};
\draw[gray,<->] (-6 + 14, 0) -- (6 + 14, 0);
\draw[black,thick,->] (0 + 14,0) -- (5 + 14,0);
\node[below] at (5 + 14, 0) {\msall $\mf{b}$};
% \draw[gray,<->] (0 + 14, -6) -- (0 + 14, 6);
% \draw[black,thick,dashed] (3 + 14,6) -- (-3 + 14,-6);
% \node[above, xshift=-0.5cm]  at (-3.2 + 14, -4) {\small $span\lp \lc \mf{a}_1, \mf{a}_2 \rc \rp$};
% \draw[black!30!green,thick,->] (0 + 14,0) -- (2 + 14,4) node[above,right] {$\mf{b}$};
% \draw[red,thick,->] (0 + 14,0) -- (1 + 14,2) node[above,right] {$\mf{a}_1$};
% \draw[blue,thick,->] (0 + 14,0) -- (-1 + 14,-2) node[above,xshift=-0.25cm] {\small{$\mf{a}_2$}};
\draw[red, thin, -latex] (1.5 + 0.1, 2) to[out=30,in=90] (5 + 14, 0 + 0.1);
\node[] at (9, 5.5) {$\mf{A}$};
\end{tikzpicture}
\end{center}
\end{frame}


\begin{frame}[t]{Geometry of the general solution}
\begin{center}
\vspace{-0.2cm}
\begin{small}
$\mf{A} = \bmx
1 & -1\\
2 & -2
\emx, \,\, \mf{b} = \bmx
2\\
4
\emx$
\end{small}
\vspace{0.5cm}

\begin{tikzpicture}[scale=0.45]
\node[] at (-5, 5) {\Large $\mb{R}^m$};
\draw[gray,<->] (-6, 0) -- (6, 0) node[right,below] {$x_1$};
\draw[gray,<->] (0, -6) -- (0, 6) node[right] {$x_2$};
\draw[black,thick,->] (0,0) -- (0,2);
\node[] at (0.5, 1.2) {$\mf{x}_p$};
\draw[black!30!green,thick,dotted] (-6,-6) -- (6,6);
\node[black!30!green,below,rotate=45] at (3, 3) {\small $\lc \mf{x}_h \, \vert \, \mf{A}\mf{x}_h = \mf{0} \rc$};
\draw[black!30!green,thick] (-6,-4) -- (4,6);
\node[black!30!green,rotate=45] at (1.5, 4) {\small $\mf{x}_p + \mf{x}_h$};

\node[] at (5 + 14, 5) {\Large $\mb{R}^n$};
\draw[gray,<->] (-6 + 14, 0) -- (6 + 14, 0);
\draw[gray,<->] (0 + 14, -6) -- (0 + 14, 6);
\draw[black,thick,dashed] (3 + 14,6) -- (-3 + 14,-6);
\node[above, xshift=-0.5cm]  at (-3.2 + 14, -4) {\small $span\lp \lc \mf{a}_1, \mf{a}_2 \rc \rp$};
\draw[black!30!green,thick,->] (0 + 14,0) -- (2 + 14,4) node[above,right] {$\mf{b}$};
\draw[red,thick,->] (0 + 14,0) -- (1 + 14,2) node[above,right] {$\mf{a}_1$};
\draw[blue,thick,->] (0 + 14,0) -- (-1 + 14,-2) node[above,xshift=-0.25cm] {\small{$\mf{a}_2$}};
\draw[red, thin, -latex] (3.1, 5) to[out=30,in=140] (2 + 14 - 0.1, 4 + 0.1);
\node[] at (9, 6) {$\mf{A}$};
\end{tikzpicture}
\end{center}
\end{frame}


\begin{frame}[t]{Linear transformations}
\begin{itemize}
    \item Linear functions $f: \mathbb{R}^m \mapsto \mathbb{R}$, 
    $$y = f\left(\mf{x}\right) = \mf{w}^{\top}\mf{x}; \,\,\, \mf{w}, \mf{x} \in \mathbb{R}^m, \,\, y \in \mathbb{R}$$

    \item Generalization of the linear function is when its range $\mathbb{R}^n$:
    $$\mf{y} = f\left(\mf{x}\right); \,\,\, \mf{x} \in \mathbb{R}^m, \,\, \mf{y} \in \mathbb{R}^n$$

    \item These can be represented as, $\mf{y} = \mf{Ax}, \,\,\, \mf{A} \in \mathbb{R}^{n \times m}$.\\

    \item Matrices can be thought of as representing a particular linear transformation.
\end{itemize}
\end{frame}


\begin{frame}[t]{Why does matrix multiplication have this strange definition?}
Consider the following two functions,
\begin{small}
\[ \mf{y} = f\lp\mf{x}\rp = \mf{A}\mf{x} \longrightarrow \begin{bmatrix}y_1 \\ y_2\end{bmatrix} = f\left(\begin{bmatrix}x_1 \\ x_2\end{bmatrix}\right) = \begin{bmatrix}ax_1 + bx_2 \\ cx_1 + dx_2\end{bmatrix} = \begin{bmatrix}a & b \\ c & d\end{bmatrix}\begin{bmatrix}x_1 \\ x_2\end{bmatrix}\]
\[ \mf{v} = g\lp\mf{u}\rp = \mf{B}\mf{u} \longrightarrow \begin{bmatrix}v_1 \\ v_2\end{bmatrix} = g\left(\begin{bmatrix}u_1 \\ u_2\end{bmatrix}\right) = \begin{bmatrix}\alpha u_1 + \beta u_2 \\ \gamma u_1 + \delta u_2\end{bmatrix} = \begin{bmatrix}\alpha & \beta \\ \gamma & \delta\end{bmatrix}\begin{bmatrix}u_1 \\ u_2\end{bmatrix}\]
\[ \begin{split}
\mf{z} = h\left(\mf{u}\right) &= f\left(g\left(\mf{u}\right)\right) = f\left(\begin{bmatrix}\alpha u_1 + \beta u_2 \\ \gamma u_1 + \delta u_2\end{bmatrix}\right) = \begin{bmatrix}a\alpha u_1 + a\beta u_2 + b\gamma u_1 + b\delta u_2 \\ c\alpha u_1 + c\beta u_2 + d\gamma u_1 + d\delta u_2 \end{bmatrix}\\
&= \begin{bmatrix}\left(a\alpha + b\gamma\right) u_1 + \left(a\beta + b\delta\right)u_2 \\ \left(c\alpha + d\gamma\right)u_1  + \left(c\beta + d\delta\right)u_2 \end{bmatrix} = \begin{bmatrix}a\alpha + b\gamma & a\beta + b\delta \\ c\alpha + d\gamma & c\beta + d\delta \end{bmatrix} \begin{bmatrix}u_1 \\ u_2 \end{bmatrix}
\end{split}
\]
\[\mf{z} = \mf{A}\lp\mf{B}\mf{u}\rp = \lp\mf{A}\mf{B}\rp\mf{u} \implies \mf{A}\mf{B} = \begin{bmatrix}a & b \\ c & d\end{bmatrix}\begin{bmatrix}\alpha & \beta \\ \gamma & \delta\end{bmatrix} = \begin{bmatrix}a\alpha + b\gamma & a\beta + b\delta \\ c\alpha + d\gamma & c\beta + d\delta \end{bmatrix}
\]
\end{small}
Matrix multiplication represents the composition of linear transformations.
\end{frame}


\begin{frame}[t]{Four Fundamental Subspaces of $\mf{A} \in \mb{R}^{n \times m}$}
\begin{itemize}
    \item $\mathcal{C}\left(\mf{A}\right)$: \textbf{Column Space of} $\mf{A}$ -- the span of the columns of $\mf{A}$.
    \[ \mathcal{C}\left(\mf{A}\right) = \left\{\mf{Ax} \left|\right. \mf{x} \in \mathbb{R}^m\right\} \subseteq \mathbb{R}^n \]

    \item $\mathcal{N}\left(\mf{A}\right)$: \textbf{Nullspace of} $\mf{A}$ -- the set of all $\mf{x} \in \mathbb{R}^m$ that are mapped to zero by $\mf{A}$.
    \[ \mathcal{N}\left(\mf{A}\right) = \left\{\mf{x} \left|\right. \mf{Ax}  = \mf{0}\right\} \subseteq \mathbb{R}^m \]
    
    \item $\mathcal{C}\left(\mf{A}^{\top}\right)$: \textbf{Row Space of} $\mf{A}$ -- the span of the rows of $\mf{A}$.
    \[ \mathcal{C}\left(\mf{A}^{\top}\right) = \left\{\mf{A}^{\top}\mf{y} \left|\right. \mf{y} \in \mathbb{R}^n\right\} \subseteq \mathbb{R}^m \]

    \item $\mathcal{N}\left(\mf{A}^{\top}\right)$: \textbf{Nullspace of} $\mf{A}^{\top}$ -- the set of all $\mf{y} \in \mathbb{R}^n$ that are mapped to zero by $\mf{A}^{\top}$.
    \[ \mathcal{N}\left(\mf{A}^{\top}\right) = \left\{\mf{y} \left|\right. \mf{A}^{\top}\mf{y}  = \mf{0}\right\} \subseteq \mathbb{R}^n \]

    This is also called the \textbf{left nullspace} of $\mf{A}$.
\end{itemize}
\end{frame}

\begin{frame}[t]{Linear Independence}
\begin{itemize}
    \item Given a set of vectors $\left\{\mf{v}_1, \mf{v}_2, \ldots \mf{v}_m\right\}, \,\,\, \mf{v}_i \in \mathbb{R}^n$, how can we determine if this set is linear independent?
    \item We need to verify, $a_1 \mf{v}_1 + a_2 \mf{v}_2 + \cdots + a_m \mf{v}_m= 0$
    \[ \begin{rcases*}\begin{bmatrix} \mf{v}_1 & \cdots & \mf{v}_m\end{bmatrix} \begin{bmatrix} a_1 \\ \vdots \\ a_m\end{bmatrix} = \begin{bmatrix} 0 \\ \vdots \\ 0\end{bmatrix} = \mf{V}\mf{a} = \mf{0}\end{rcases*} \mathcal{N}\left(\mf{V}\right) = \left\{\mf{0}\right\}, \,\,\, rank\left(\mf{V}\right) = n  \]
    \item This is also equivalent to saying that when the $rank \left(\mf{A}\right) = n \implies$ the columns of $\mf{A}$ form an independent set of vectors.
    \item When do the rows of $\mf{A}$ form an independent set?
    \item What about both rows and columns? When does that happen?
\end{itemize}
\end{frame}


\begin{frame}[t]{Dimension of the four fundamental subspaces}
\begin{itemize}
    \item \textbf{Column space} $C(\mf{A})$
    \begin{itemize}
        \item $\dim \,C(\mf{A}) = rank\left(\mf{A}\right) = r$
    \end{itemize}
    \item \textbf{Nullspace} $N(\mf{A})$
    \begin{itemize}
        \item $\dim \,N(\mf{A}) = n-r$
    \end{itemize}
    \item \textbf{Row space} $C(\mf{A}^{\top})$
    \begin{itemize}
        \item $\dim \,C(\mf{A}^{\top}) = rank\left(\mf{A}^{\top}\right) = rank\left(\mf{A}\right) = r$
    \end{itemize}
    \item \textbf{Left Nullspace} $N(\mf{A}^{\top})$
    \begin{itemize}
        \item $\dim \,N(\mf{A}^{\top}) = m-r$
    \end{itemize}
\end{itemize}
\end{frame}


\begin{frame}[t]{Matrix Inverse}
\begin{small}
\begin{itemize}
    \item Consider the square matrix $\mf{A} \in \mathbb{R}^{n \times n}$. $\mf{B} \in \mathbb{R}^{n \times n}$ is the inverse of $\mf{A}$, if $\mf{AB} = \mf{BA} = \mf{I}_n$, and $\mf{B}$ is represented as $\mf{A}^{-1}$.
    \item Not all matrices have inverses. A matrix with an inverse is called \textbf{non-singular}, otherwise it is called \textbf{singular}.
    \item For a non-singular matrix $\mf{A}$, $\mf{A}^{-1}$ is unique. $\mf{A}^{-1}$ is both the left and right inverse.
    \item A matrix $\mf{A}$ has an inverse, if and only if $\mf{A}$ is full rank, i.e. $rank\left(\mf{A}\right) = n$
    \item $\mf{Ax} = \mf{b}$ can be solved as follows, $\mf{x} = \mf{A}^{-1}\mf{b}$. \textit{It is never solved like this in practice.}
    \item Inverse of product of matrices, $\left(\mf{AB}\right)^{-1} = \mf{B}^{-1}\mf{A}^{-1}$.
    \item $\left(\mf{A}^{-1}\right)^{-1} = \mf{A}$ and $\left(\mf{A}^{-1}\right)^{\top} = \left(\mathbf{A}^{T}\right)^{-1}$
\end{itemize}
\end{small}
\end{frame}

\end{document}