% Copyright 2004 by Till Tantau <tantau@users.sourceforge.net>.
%
% In principle, this file can be redistributed and/or modified under
% the terms of the GNU Public License, version 2.
%
% However, this file is supposed to be a template to be modified
% for your own needs. For this reason, if you use this file as a
% template and not specifically distribute it as part of a another
% package/program, I grant the extra permission to freely copy and
% modify this file as you see fit and even to delete this copyright
% notice. 

\documentclass[aspectratio=169]{beamer}
%\documentclass{beamer}

\setbeamersize{text margin left=2mm, text margin right=2mm}

\defbeamertemplate{headline}{my header}{%
\vskip1pt%
\makebox[0pt][l]{\,\insertshortauthor}%
\hspace*{\fill}\insertshorttitle/\insertshortsubtitle\hspace*{\fill}%
\llap{\insertpagenumber/\insertpresentationendpage\,}
}
\setbeamertemplate{headline}[my header]

\usepackage{soul}
\usepackage{tkz-euclide}
\usetikzlibrary{calc}
\usepackage[]{algorithm2e}
\usepackage{changepage}
%\usetikzlibrary{arrows.meta}

%\setbeamertemplate{itemize items}{-}

%\usepackage{helvet}
\usefonttheme{professionalfonts} % using non standard fonts for beamer
%\usefonttheme{serif} % default family is serif
%\usepackage{fontspec}
%\setmainfont{Liberation Serif}

% There are many different themes available for Beamer. A comprehensive
% list with examples is given here:
% http://deic.uab.es/~iblanes/beamer_gallery/index_by_theme.html
% You can uncomment the themes below if you would like to use a different
% one:
%\usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
%\usetheme{Berkeley}
%\usetheme{Berlin}
%\usetheme{Boadilla}
%\usetheme{boxes}
%\usetheme{CambridgeUS}
%\usetheme{Copenhagen}
%\usetheme{Darmstadt}
%\usetheme{default}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
%\usetheme{Madrid}
%\usetheme{Malmoe}
%\usetheme{Marburg}
%\usetheme{Montpellier}
%\usetheme{PaloAlto}
%\usetheme{Pittsburgh}
%\usetheme{Rochester}
%\usetheme{Singapore}
%\usetheme{Szeged}
%\usetheme{Warsaw}



\title{Linear Control and Estimation}

% A subtitle is optional and this may be deleted
\subtitle{Vectors}

\author{Sivakumar Balasubramanian}
% - Give the names in the same order as the appear in the paper.
% - Use the \inst{?} command only if the authors have different
%   affiliation.

\institute[Christian Medical College] % (optional, but mostly needed)
{
  \inst{}%
  Department of Bioengineering\\
  Christian Medical College, Bagayam\\
  Vellore 632002
}
% - Use the \inst command only if there are several affiliations.
% - Keep it simple, no one is interested in your street address.

\date{}
% - Either use conference name or its abbreviation.
% - Not really informative to the audience, more for people (including
%   yourself) who are reading the slides online

\subject{Lecture notes on linear control and estimation}
% This is only inserted into the PDF information catalog. Can be left
% out. 

% If you have a file called "university-logo-filename.xxx", where xxx
% is a graphic format that can be processed by latex or pdflatex,
% resp., then you can add a logo as follows:

% \pgfdeclareimage[height=0.5cm]{university-logo}{university-logo-filename}
% \logo{\pgfuseimage{university-logo}}

% Delete this, if you do not want the table of contents to pop up at
% the beginning of each subsection:
\AtBeginSubsection[]
{
  \begin{frame}<beamer>{Outline}
    \tableofcontents[currentsection,currentsubsection]
  \end{frame}
}

% Let's get started
\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}[t]{References}
\begin{itemize}
\item S Boyd, Applied Linear Algebra: Chapters 1, 2, 3 and 5.
\item G Strang, Linear Algebra: Chapters 
\end{itemize}
\end{frame}

\begin{frame}[t]{Vectors}
\begin{itemize}
\item \textbf{Vectors} are ordered list of numbers (scalars). $v = \begin{bmatrix} 1.2 \\ -0.1 \\ 2.14 \\ 9.0 \\ -1.24 \end{bmatrix}$
\item Scalars can be any \textit{field} $\mathbb{R}, \mathbb{C}, \mathbb{Z}, \mathbb{Q}$.
\item We will typically only encouter on $\mathbb{R}$ in this course.
\item Individual elements of a vector $v$ are indexed. $i^{th}$ elements is referred to as $v_i$.
\item \textit{Dimension} or \textit{size} of a vector is number of elements in the vector.
\item Set of $n$-real vectors is denoted by $\mathbb{R}^n$ (similarly, $\mathbb{C}^n$)
\item Vectors $a$ and $b$ are equal, if
\begin{itemize}
\item both have the same size; and
\item $a_i = b_i, i \in \left\{1, 2, 3, \ldots n\right\}$
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[t]{Vectors}

\begin{itemize}
\item \textbf{Unit vector} $e_1 = \begin{bmatrix} 1 \\ 0 \\ 0 \\ \vdots \\ 0 \end{bmatrix}$ \textbf{Zero vector} $0 = \begin{bmatrix} 0 \\ 0 \\ 0 \\ \vdots \\ 0 \end{bmatrix}$ \textbf{One vector} $\mathbf{1} = \begin{bmatrix} 1 \\ 1 \\ 1 \\ \vdots \\ 1 \end{bmatrix}$
\item Geometrically, real $n$-vectors can be thought of as points in $\mathbb{R}^n$ space.\\
\begin{center}
\begin{tikzpicture}
\node (A) at (0,0) {$0$};
\node (B) at (1,2) {$v$};
\draw[thick,->] (A) -- (B);
\end{tikzpicture}
\end{center}
\end{itemize}
\end{frame}

\begin{frame}[t]{Vectors}
\begin{columns}[T]
\begin{column}{0.5\textwidth}
\begin{itemize}
\item \textbf{Vector scaling}: Multiplication of a scalar and a vector.
\begin{small}
$$ w = av = a\begin{bmatrix} v_1 \\ v_2 \\ v_3 \\ \vdots \\ v_n \end{bmatrix} = \begin{bmatrix} av_1 \\ av_2 \\ av_3 \\ \vdots \\ av_n \end{bmatrix} \,\, a \in \mathbb{R}; \,\, w, v \in \mathbb{R}^n$$
\end{small}
\end{itemize}
\begin{center}
\begin{tikzpicture}[scale=0.6]
\path coordinate (a) at (0,0)
      coordinate (b) at (1.0,2.0)
      coordinate (c) at ($(a)!0.7!(b)$);
\draw[thick,blue,->] (a)  -> (b);
\draw[thick,red,->] (a)  -> (c);
\node[below right] at (a){$0$}; 
\node[blue,above left] at (b){$v$};
\node[red,above left] at (c){$0.7v$};  
\end{tikzpicture}\hspace{1cm}
\begin{tikzpicture}[scale=0.6]
\path coordinate (a) at (0,0)
      coordinate (b) at (1.0,2.0)
      coordinate (c) at ($(a)!-1.1!(b)$);
\draw[thick,red,->] (a)  -> (c);
\draw[thick,blue,->] (a)  -> (b);
\node[below right] at (a){$0$}; 
\node[blue,above left] at (b){$v$};
\node[red,above left] at (c){$-1.1v$};  
\end{tikzpicture}
\end{center}
\end{column}
\begin{column}{0.5\textwidth}
\textbf{Properties}
\begin{itemize}
\item Scalar multiplication is \textit{commutative}.
$$ \alpha v = v \alpha $$
\item Scalar multiplication is \textit{associative}.
$$ \left(\alpha \beta\right) v = \alpha \left(\beta v\right) $$
\item Scalar multiplication is \textit{distributive}.
$$ \left(\alpha + \beta\right) v = \alpha v + \beta v $$
\end{itemize}
\end{column}
\end{columns}
\end{frame}

\begin{frame}[t]{Vectors}
\begin{columns}[T]
\begin{column}{0.5\textwidth}
\begin{itemize}
\item \textbf{Vector addition}: Adding two vectors of the same dimension, element by element.
\begin{small}
$$ u + v = \begin{bmatrix} u_1 \\ u_2 \\ u_3 \\ \vdots \\ u_n \end{bmatrix} + \begin{bmatrix} v_1 \\ v_2 \\ v_3 \\ \vdots \\ v_n \end{bmatrix}= \begin{bmatrix} u_1 + v_1 \\ u_2 + v_2 \\ u_3 + v_3 \\ \vdots \\ u_n + v_n \end{bmatrix} \,\, u, v \in \mathbb{R}^n$$
\end{small}
\end{itemize}
\begin{center}
\begin{tikzpicture}[scale=0.8]
\path coordinate (a) at (0,0)
      coordinate (b) at (1.0,-2.0)
      coordinate (c) at (1.5,1.2)
      coordinate (d) at ($(b)!0.5!(c)$)
      coordinate (e) at ($(a)!2.0!(d)$);
\draw[thick,blue,->] (a)  -> (b);
\draw[thick,red,->] (a)  -> (c);
\draw[thick,green,->] (a)  -> (e);
\node[above left] at (a){$0$}; 
\node[blue,above right] at (b){$u$};
\node[red,above left] at (c){$v$};
\node[green,above right] at (e){$u+v$};
\end{tikzpicture}
\end{center}
\end{column}
\begin{column}{0.5\textwidth}
\textbf{Properties}
\begin{itemize}
\item Vector addition is \textit{commutative}.
$$ a + b = b + a $$
\item Vector addition is \textit{associative}.
$$ \left(a + b\right) + c =	 a + \left(b + c \right) $$
\item Zero vector has no effect.
$$ a + 0 = a $$
\item Subtraction of vectors. 
$$ a + (-1)a = a - a = 0 $$
\end{itemize}
\end{column}
\end{columns}
\end{frame}

\begin{frame}[t]{Vector spaces}
\begin{itemize}
\item A set of that is closed under \textit{vector addition} and \textit{vector scaling}. 
\item For a set to be a vector space, it must satisfy the followng properties: $x, y, x \in V$
\begin{itemize}
\item \textit{Commutativity}: $x+ y = y + x$
\item \textit{Associativity of vector addition}: $(x + y) + z = x + (y + z)$
\item \textit{Additive identity}: $x + 0 = 0 + x = 0$ $\left(0 \in V\right)$
\item \textit{Additive inverse}: $\exists -x \in V, x + (-x) = 0$
\item \textit{Associativity of scalar multiplication}: $\alpha\left(\beta x\right) = \left(\alpha\beta x\right)$
\item \textit{Distributivity of scalar sums}: $\left(\alpha + \beta\right)x = \alpha x + \beta x$
\item \textit{Distributivity of vector sums}: $\alpha\left(x + y\right) = \alpha x + \alpha y$
\item \textit{Scalar multiplication identity}: $1x = x$
\end{itemize}
\item $W$ is a subspace of $V$, if $W \subseteq V$ and $W$ is itself a vector space.
\item We will mostly deal with $\mathbb{R}^n$ vectors spaces in this course.
\end{itemize}
\end{frame}

\begin{frame}[t]{Inner product}
\begin{itemize}
\item Standard inner product between two vectors $x, y \in \mathbb{R}^n$ is defined as,
$$ x^Ty = \begin{bmatrix} x_1 \\ x_2 \\ x_3 \\ \vdots \\ x_n \end{bmatrix}^T \begin{bmatrix} y_1 \\ y_2 \\ y_3 \\ \vdots \\ y_n \end{bmatrix} = \sum_{i=1}^{n}x_iy_i$$
\item \textbf{Properties}
\begin{itemize}
\item \textit{Commutative}: $x^Ty = y^Tx$
\item \textit{Associativity with scalar multiplication}: $\left(\alpha x\right)^Ty = \alpha \left(x^Ty\right)$
\item \textit{Distributivity with vector addition}: $\left(x + y\right)^Tz = x^Tz + y^Tz$
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[t]{Norm}
\begin{columns}[T]
\begin{column}{0.65\textwidth}
\begin{itemize}
\item Norm is a measure of the size of a vector.
\item \textit{Euclidean norm} of a $n$-vector $x \in \mathbb{R}^n$ is defined as, $\left\Vert x\right\Vert_2 = \sqrt{x^Tx} = \sqrt{\sum_{i=1}^{n}x_i^2}$.
\item $\left\Vert x \right\Vert_2$ is a measure of the length of the vector $x$.
\item \textbf{Properties}
\begin{itemize}
\item \textit{Non-negativity}. $\left\Vert x \right\Vert \geq 0$
\item \textit{Non-negative homogeneity}. $\left\Vert \beta x \right\Vert = \left|\beta\right|\left\Vert x \right\Vert, \, \beta \in \mathbb{R}$
\item \textit{Triangle inequality}. $\left\Vert x + y\right\Vert \leq \left\Vert x\right\Vert + \left\Vert y\right\Vert$
\item \textit{Definiteness}. $\left\Vert x\right\Vert = 0 \iff x = 0$
\end{itemize}
\item Any function of the form $\left\Vert\bullet\right\Vert: \mathbb{R}^n \longrightarrow \mathbb{R}$ is a valid norm.
\item $p$-norm: $\left\Vert x \right\Vert_p = \left(\sum_{i=1}^{n}\left|x_i\right|^p\right)^{\frac{1}{p}}$
\item Norm of difference between two vectors is a measure of the distance between the vectors. $d = \left\Vert x - y \right\Vert_2$.
\end{itemize}
\end{column}
\begin{column}{0.35\textwidth}
\begin{tikzpicture}[scale=0.7]
	\def \r{0.9}
	\def \ya{0}
	\def \yb{-2.5}
	\def \yc{-5}
	\def \yd{-7.5}
	
	\draw[->] (3.9,\ya) -- (6.1,\ya) node[black,right,xshift=0.cm, yshift=0.cm] {\small{$\left\Vert x \right\Vert_1 = \sum_{i=1}^{n}\left|x_i\right|$}};
	\draw[->] (5,\ya-1.1) -- (5,\ya+1.1);
	\draw[thick, gray] (5-\r,0)--(5,\r)--(5+\r,0)--(5,-\r)--cycle;
	
	\draw[->] (3.9,\yb) -- (6.1,\yb) node[black,right,xshift=0.cm, yshift=0.cm] {\small{$\left\Vert x \right\Vert_2 = \left(\sum_{i=1}^{n}x_i^2\right)^{\frac{1}{2}}$}};
	\draw[->] (5,\yb-1.1) -- (5,\yb+1.1);
	\draw [thick, gray](5,-2.5) circle (\r);
	
	\draw[->] (3.9,\yc) -- (6.1,\yc) node[black,right,xshift=0.cm, yshift=0.cm] {\small{$\left\Vert x \right\Vert_\infty = \max_{1\leq i\leq n}\left|x_i\right|$}};
	\draw[->] (5,\yc-1.1) -- (5,\yc+1.1);
	\draw[thick, gray] (5-\r,-5-\r)--(5-\r,-5+\r)--(5+\r,-5+\r)--(5+\r,-5-\r)--cycle;

	\draw[->] (3.9,\yd) -- (6.1,\yd) node[black,right,xshift=0.cm, yshift=0.cm] {\small{$\left\Vert x \right\Vert_p = \left(\sum_{i=1}^{n}x_i^p\right)^{\frac{1}{p}}$}};
	\draw[->] (5,\yd-1.1) -- (5,\yd+1.1);
	\draw[thick, gray,scale=1,domain=0:90,samples=100,smooth,variable=\t]
	plot({-\r*sqrt(cos(\t))+5},{\r*sqrt(sin(\t))-7.5});
	\draw[thick, gray,scale=1,domain=0:90,samples=100,smooth,variable=\t]
	plot({-\r*sqrt(cos(\t))+5},{-\r*sqrt(sin(\t))-7.5});
	\draw[thick, gray,scale=1,domain=0:90,samples=100,smooth,variable=\t]
	plot({\r*sqrt(cos(\t))+5},{-\r*sqrt(sin(\t))-7.5});
	\draw[thick, gray,scale=1,domain=0:90,samples=100,smooth,variable=\t]
	plot({\r*sqrt(cos(\t))+5},{\r*sqrt(sin(\t))-7.5});                 
\end{tikzpicture}
\end{column}
\end{columns}
\end{frame}

\begin{frame}[t]{Angle between vectors}
\begin{columns}[T]
\begin{column}{0.65\textwidth}
\begin{itemize}
\item \textbf{Cauchy-Schwartz Inequality}: $\left|x^Ty\right| \leq \left\Vert x \right\Vert_2 \left\Vert y \right\Vert_2$
\item \textbf{Angle between two vectors} is defined as, $\theta = \arccos \left(\frac{x^Ty}{\left\Vert x \right\Vert_2 \left\Vert y \right\Vert_2}\right) \implies  x^Ty = \left\Vert x \right\Vert_2 \left\Vert y \right\Vert_2 \cos \theta$
\item Inner product measures the projection of one vector on the other.
\end{itemize}
\end{column}
\begin{column}{0.35\textwidth}
\begin{tikzpicture}[scale=1.0]
	\draw[blue,thick,->] (0, 0) -- (1.8, 1.8) node[black, above, right]{$y$};
	\draw[red,thick,->] (0, 0) -- (3.3, 0.0) node[black, above, right]{$x$};
	\draw[black,domain=0:45] plot({0.75 * cos(\x)}, {0.75 * sin(\x)});
	\node[black,xshift=0.9cm,yshift=0.4cm] {$\theta$};
	\draw[thick,dashed] (1.8,1.8) -- (1.8,0);
	\draw[gray,black] (0,-0.1) -- (0,-0.3);
	\draw[gray,black] (1.8,-0.1) -- (1.8,-0.3);
	\draw[<->] (0,-0.2) -- (1.8,-0.2) node[midway,below] {$x^Ty$};
\end{tikzpicture}
\end{column}
\end{columns}
\begin{center}
\begin{tikzpicture}[scale=1.0]
	\draw[blue,thick,->] (0, 0) -- (1.1, 1.1) node[black, above, right]{$y$};
	\draw[red,thick,->] (0, 0) -- (2.5, 0.0) node[black, above, right]{$x$};
\end{tikzpicture}
\begin{tikzpicture}[scale=1.0]
	\draw[blue,thick,->] (0, 0) -- (-0.1, 1.25) node[black, above, right]{$y$};
	\draw[red,thick,->] (0, 0) -- (2.5, 0.2) node[black, above, right]{$x$};
\end{tikzpicture}
\begin{tikzpicture}[scale=1.0]
	\draw[blue,thick,->] (0, 0) -- (-1.1, 1.5) node[black, above, left]{$y$};
	\draw[red,thick,->] (0, 0) -- (1.8, 0.0) node[black, above, right]{$x$};
\end{tikzpicture}
\begin{tikzpicture}[scale=0.5]
	\draw[red,thick,->] (0, 0) -- (-2.0, 3.0) node[black, above, right]{$x$};
	\draw[blue,thick,->] (0, 0) -- (-1.0, 1.5) node[black, above, right]{$y$};
\end{tikzpicture}
\begin{tikzpicture}[scale=0.5]
	\draw[red,thick,->] (0, 0) -- (-2.0, 3.0) node[black, above, right]{$x$};
	\draw[blue,thick,->] (0, 0) -- (1.0, -1.5) node[black, above, right]{$y$};
\end{tikzpicture}
\end{center}
\end{frame}

\begin{frame}[t]{Linear independence}
\begin{itemize}
\item A collection of vectors $\left(x_1, x_2, x_3, \ldots x_n \right), \,\,\, x_i \in \mathbb{R}^m \,\,\, i \in\left\{1, 2, 3, \ldots n\right\}$ is called \textit{linear dependent} if,
$$\sum_{i=1}^na_ix_i = 0, \text{ hold for some } a_1, a_2, \ldots a_n \in \mathbb{R}, \text{ such that } \exists a_i \neq 0$$
\item Another way to state this: A collection of vectors is \textit{linear dependent} if at least one of the vectors in the collection can be expressed as a linear combination of the other vectors in the set, i.e.
$$x_i = -\sum_{j=1, j\neq i}^{n}\left(\frac{a_j}{a_i}\right)x_j$$
\item A collection of vectors is \textit{linear independent} if it is \textbf{not} \textit{linearly dependent}.
$$ \sum_{i=1}^na_ix_i = 0 \implies a_1=a_2=a_3\ldots=a_n = 0$$
\end{itemize}
\end{frame}

\begin{frame}[t]{Basis}
\begin{itemize}
\item Consider a vector $y = \sum_{i=1}^na_ix_i$. What can we say about the coefficients $a_i$s when the collection ${x_i}_{i=1}^n$ is,
\begin{itemize}
\item linearly independent $\implies a_i$s are \textit{unique}.
\item linearly dependent $\implies a_i$s are not \textit{unique}.
\end{itemize}
\item \textbf{Independence-Dimension inequality}: What is the maximum possible size of a linearly independent collection?
\center{\textit{A linear independent collection of n-vectors can at most have n vectors.}}
\end{itemize}
Consider $\mathbb{R}^2$ vector space. $x_1=\begin{bmatrix}1\\0\end{bmatrix}, \,\, x_2=\begin{bmatrix}1\\1\end{bmatrix} \,\, x_3 = \begin{bmatrix}-1, 1\end{bmatrix}$.
\begin{center}
\begin{tikzpicture}[scale=1.5]
  \draw[thick, blue, ->] (0,0) -- (1, 0) node[right]{$x_1$};
  \draw[thick, blue, ->] (0,0) -- (1, 1) node[below right]{$x_2$};
  \draw[thick, blue, ->] (0,0) -- (-1, 1) node[above]{$x_3 = -2x_1 + x_2$};
  \draw[dashed, red, ->] (0,0) -- (-2, 0) node[left]{$-2x_1$};
  \draw[dotted, thick, black] (-2,0) -- (-1, 1);
  \draw[dotted, thick, black] (-1,1) -- (1, 1);
\end{tikzpicture}
\end{center}
\end{frame}

\begin{frame}[t]{Basis}
\begin{itemize}
\item A linearly independent collection of $n$ $n$-vector is called a \textit{basis}. In particular, it is a basis of $\mathbb{R}^n$.
\item Any $n$-vector can be represented as a \textit{unique} linear combination of the elements of the basis.
\item Consider the basis $\left\{x_i\right\}_{i=1}^{n}$. A $n$-vector $y$ can be represented as a linear combination of $x_i$s, $y = \sum_{i=1}^n a_ix_i$. This is called the \textit{expansion of} $y$ in the $\left\{x_i\right\}_{i=1}^n$ basis.
\item The numbers $a_i$ are called the \textit{coefficients} of the expansion of $y$ in the $\left\{x_i\right\}_{i=1}^n$ basis.
\item \textbf{Orthonormal vectors}: A collections of vectors $\left\{x_i\right\}_{i=1}^n$ is \textit{(mutually) orthogonal} is $x_i \perp x_j$ for all $i, j \in \left\{1, 2, 3, \ldots n\right\}$ and $i \neq j$.
\item This collection is called \textit{orthonormal} if its elements are all of unit length $\left\Vert x_i \right\Vert_2 = 1$ for all $i \in \left\{1, 2, 3, \ldots n\right\}$.
\end{itemize}
\[ x_i^Tx_j = \begin{cases} 
      0 & i \neq j \\
      1 & i = j 
   \end{cases}
\]
\end{frame}

\begin{frame}[t]{Basis}
\begin{itemize}
\item An orthonormal collection of vectors is linearly independent.
\item Consider an orthonormal basis $\left\{x_i\right\}_{i=1}^{n}$. The expansion of a vector $y$ is given by,
\[ y = a_1x_1 + a_2x_2 + a_3x_3 + \ldots + a_nx_n \]
\[ x_i^Ty = a_1x_i^Tx_1 + a_2x_i^Tx_2 + a_3x_i^Tx_3 + \ldots + a_nx_i^Tx_n = a_i\]
\item Thus, we can rewrite this as,
\[ y = \left(y^Tx_1\right)x_1 + \left(y^Tx_2\right)x_2 + \left(y^Tx_3\right)x_3 + \ldots + \left(y^Tx_1\right)x_n \]
\end{itemize}
\begin{center}
\begin{tikzpicture}[scale=2.5]
  \foreach \x in {-0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2}
    \draw[gray, thin, densely dashed] (\x,-0.2)--(\x,1.2);
  \foreach \x in {-0.2, 0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2}
    \draw[gray, thin, densely dashed] (-0.4,\x)--(1.2,\x);
  \draw[thick, densely dashed, black] (0.4,0.8) -- (0.4,0);
  \draw[thick, densely dashed, black] (0.4,0.8) -- (0.,0.8);
  \draw[very thick, blue, ->] (0,0) -- (1,0) node[right]{$x_1$};
  \draw[very thick, blue, ->] (0,0) -- (0,1) node[above right]{$x_2$};
  \draw[very thick, red, ->] (0,0) -- (0.4,0.8) node[above]{$y$};
\end{tikzpicture}
\end{center}
\end{frame}

\begin{frame}[t]{Gram-Schmidt algorithm}
\begin{itemize}
\item Given a collection of vectors, how can we determine if it is linearly independent? $\longrightarrow$ Gram-Schmidt algorithm
\item If a collection $\left\{x_i\right\}_{i=1}^{n}$ is linearly independent, the algorithm produces an orthonormal basis for the space spanned by the set.
\end{itemize}
\begin{small}
\begin{center}
\fbox{\begin{minipage}{35em}
\begin{algorithm}[H]
  \KwData{$\left\{x_i\right\}_{i=1}^n$}
  \KwResult{True/False indicating linear independence of $\left\{x_i\right\}_{i=1}^n$ \& if True returns an orthogonal basis $\left\{q_i\right\}_{i=1}^{n}$.}
  \For{$i = 1, 2, \ldots n$}{
  1. $\tilde{q}_i = x_i - \sum_{j=1}^{i-1}\left(q_j^Tx_i\right)q_i$ $\longrightarrow$(\textbf{Orthogonalization step})\;
  2. \textbf{If} $\tilde{q}_i=0$ \textbf{then} quit\;
  3. $q_i = \tilde{q}_i / \left\Vert\tilde{q}_i\right\Vert $ $\longrightarrow$(\textbf{Normalization step})\;
  }
  \textbf{return} $\left\{q_i\right\}_{i=1}^n$\;
\end{algorithm}
\end{minipage}}
\end{center}
\end{small}
\end{frame}

\begin{frame}[t]{Gram-Schmidt algorithm}
\begin{itemize}  
\item When algorithm quits before $i=n$, then the collection $\left\{x_i\right\}_{i=1}^n$ is linearly dependent.
\item At each iteration step $i$, we have an orthonormal set $\left\{q_j\right\}_{j=1}^i$. $q_l^Tq_m = \begin{cases} 1 & l = m\\ 0 & l \neq m \end{cases}$
\item $x_i$ is a linear combination of $\left\{q_j\right\}_{j=1}^i$.
\item $q_i$ is a linear combination of $\left\{x_j\right\}_{j=1}^i$.
\end{itemize}
\end{frame}

\end{document}